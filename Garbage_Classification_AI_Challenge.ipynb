{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "913a9362",
      "metadata": {
        "id": "913a9362"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ag9QJ-1YFuG8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag9QJ-1YFuG8",
        "outputId": "4ee811b1-4ced-470c-ab08-a140be5fe266"
      },
      "outputs": [],
      "source": [
        "!wget -O \"garbage_classification_ai_challenge-dataset.zip\" \"https://dockship-job-models.s3.ap-south-1.amazonaws.com/c6275a98adb6ad948b76a5a089e37376?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20211221%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20211221T042708Z&X-Amz-Expires=1800&X-Amz-Signature=8796a6ae10976f6bedf1c19c92549907ba0a6c3ef11659a1510a8d57ecd8b718&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22garbage_classification_ai_challenge-dataset.zip%22\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2jpaqtsGfhj",
      "metadata": {
        "id": "b2jpaqtsGfhj"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/garbage_classification_ai_challenge-dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0e59431a",
      "metadata": {
        "id": "0e59431a"
      },
      "outputs": [],
      "source": [
        "classes = os.listdir(\"/content/TRAIN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ea8a1086",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea8a1086",
        "outputId": "b6b18a3c-e945-426e-ea09-60ce4ce3c359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['metal', 'cardboard', 'paper', 'plastic', 'glass', 'trash']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f255442f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f255442f",
        "outputId": "3c46793e-4622-4552-9fb6-d8ffd793290a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2001 files belonging to 6 classes.\n",
            "Using 1601 files for training.\n"
          ]
        }
      ],
      "source": [
        "train_set = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/TRAIN\",\n",
        "    validation_split = 0.2,\n",
        "    subset = \"training\",\n",
        "    seed = 1337,\n",
        "    image_size = (224, 224),\n",
        "    batch_size= 32,\n",
        "    label_mode = \"categorical\",\n",
        "    class_names = classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0d1ed89a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d1ed89a",
        "outputId": "bcdacdd8-4b1e-4be2-ff82-a1aaa3505055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2001 files belonging to 6 classes.\n",
            "Using 400 files for validation.\n"
          ]
        }
      ],
      "source": [
        "val_set = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/TRAIN\",\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = 1337,\n",
        "    image_size = (224, 224),\n",
        "    batch_size= 32,\n",
        "    label_mode = \"categorical\",\n",
        "    class_names = classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726b7a61",
      "metadata": {
        "id": "726b7a61"
      },
      "outputs": [],
      "source": [
        "baseModel = Xception(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d5227545",
      "metadata": {
        "id": "d5227545"
      },
      "outputs": [],
      "source": [
        "headModel = baseModel.output\n",
        "x = Flatten()(headModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ec59f6ae",
      "metadata": {
        "id": "ec59f6ae"
      },
      "outputs": [],
      "source": [
        "pred = Dense(6, activation = \"softmax\")(x)\n",
        "model = Model(inputs = baseModel.input, outputs = pred)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "af3cb9b1",
      "metadata": {
        "id": "af3cb9b1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yvkuT-ZycqzR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvkuT-ZycqzR",
        "outputId": "cd18a2e9-c922-48cd-dc68-e8027d9cc034"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('model-{epoch:03d}.model', monitor = 'val_loss', verbose = 0, save_best_only = True, mode = 'auto')\n",
        "history=model.fit(train_set, validation_data = val_set, epochs = 10, batch_size = 64, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9ICUfK_3KnvZ",
      "metadata": {
        "id": "9ICUfK_3KnvZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "TLfM76__sA75",
      "metadata": {
        "id": "TLfM76__sA75"
      },
      "outputs": [],
      "source": [
        "model = load_model('model-014.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "iyjca6nosDkx",
      "metadata": {
        "id": "iyjca6nosDkx"
      },
      "outputs": [],
      "source": [
        "img_name = []\n",
        "cat = []\n",
        "for img in os.listdir(\"/content/TEST\"):\n",
        "  img_path = os.path.join(\"/content/TEST\", img)\n",
        "  image = load_img(img_path, target_size = (224, 224))\n",
        "  image = img_to_array(image)\n",
        "  image = np.expand_dims(image, axis = 0)\n",
        "  result = model.predict(image)\n",
        "  pred = classes[np.argmax(result)]\n",
        "\n",
        "  img_name.append(img)\n",
        "  cat.append(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "sqBTyZUUuP9l",
      "metadata": {
        "id": "sqBTyZUUuP9l"
      },
      "outputs": [],
      "source": [
        "dist = {\"Filename\":img_name, \"Labels\": cat}\n",
        "df = pd.DataFrame(dist)\n",
        "df.to_csv(\"output.csv\", index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Garbage Classification AI Challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
